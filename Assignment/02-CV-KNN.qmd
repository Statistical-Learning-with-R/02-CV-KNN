---
title: "Assignment 2: Cross-Validation and K-Nearest-Neighbors"
author: "Stat 434"
output: rmdformats::readthedown
---

```{r, include = FALSE}
templar::versions()
```

```{r, include = FALSE}
set.seed(3849)
```

%%%
version: instructions

# Instructions

You will submit an HTML document to Canvas as your final version.

You may work with **one** other person for this assignment.  Make sure both your names are on the HTML document, and you should **both** upload a copy of the assignment to Canvas.

Your document should show your code chunks/cells as well as any output.  Make sure that only relevant output is printed.  Do not, for example, print the entire dataset in your final knitted file.

Your document should also be clearly organized, so that it is easy for a reader to find your answers to each question.

There may be a small penalty for submissions that are difficult to read or navigate.

%%%



```{r, version = "none", include = FALSE, eval = FALSE}
library(tidyverse)
library(tidymodels)

whr <- read_csv(here::here("Assignments", "data", "world-happiness-report.csv"))
whr2 <- read_csv(here::here("Assignments", "data", "world-happiness-report-2021.csv"))

ref <- whr2 %>% select(`Country name`, `Regional indicator`)

whr %>%
  left_join(ref) %>%
  janitor::clean_names() %>% 
  filter(year < 2020) %>%
  select(country_name, 
         regional_indicator,
         life_ladder,
         log_gdp_per_capita,
         healthy_life_expectancy_at_birth, 
         social_support,
         freedom_to_make_life_choices,
         generosity,
         perceptions_of_corruption,
         year) %>%
  rename(healthy_life_expectancy = healthy_life_expectancy_at_birth,
         happiness_score = life_ladder) %>%
  write_csv(here::here("Assignments", "data","whr_clean.csv"))
  

whr2 %>% 
  janitor::clean_names() %>%
  select(country_name, 
         regional_indicator,
         logged_gdp_per_capita,
         healthy_life_expectancy, 
         social_support,
         freedom_to_make_life_choices,
         generosity,
         perceptions_of_corruption) %>%
  rename(log_gdp_per_capita = logged_gdp_per_capita) %>%
  mutate(
    year = 2020
  ) %>%
  write_csv(here::here("Assignments", "data","whr_2020.csv"))
  
```

# The Data

```{r, message = FALSE, warning = FALSE}
library(tidyverse)
library(tidymodels)
```



The dataset we will use for this assignment pertains to the [World Happiness Report](https://worldhappiness.report/).  The following dataset contains information about six measurements of well-being in a country:

* The Gross Domestic Product (GDP) per capita  *(dollars; log scale)*

* The life expectancy at birth of a healthy citizen *(years)*

* The social support of friends and family *(scale of 0-1, based on surveys)*

* The freedom to make life choices in the society *(scale of 0-1, based on surveys)*

* The generosity of the society *(scale of -1 to 1, based on surveys)*

* Perception of corruption in government of the country *(scale of 0-1, based on surveys)*


The data also records the year of measurement and the region of the country.


Although it is not a formal question on this assignment, you should begin by reading in the dataset and briefly exploring and summarizing the data, and by adjusting any variables that need cleaning.

```{r, echo = TRUE}
whr <- read_csv("https://www.dropbox.com/s/aau732ery3uujct/whr_clean.csv?dl=1")

```

```{r, version = "answer_key"}

whr <- whr %>%
  mutate(
    regional_indicator = factor(regional_indicator)
  ) 
```


# Part One: Happiness Scores

The World Happiness Report uses their own, privately-determined formula to combine the six measures of life quality into a "happiness score".  The dataset also contains this score, on a scale from 1 to 10.


*Hint: The following two questions require you to fit only one model each.*


#### Q1:  Happiness over time

Is the happiness in the world changing linearly over time?  Fit a simple linear model and interpret the results to address this question.

```{r, version = "answer_key"}
lin_mod <- linear_reg() %>%
  set_mode("regression") %>%
  set_engine("lm")

whr_fit <- lin_mod %>%
  fit(happiness_score ~ year, data = whr)

whr_fit$fit %>% summary()

```

%%%
version: answer_key

No, the model with only `year` as a predictor is not significant.
%%%


Was the happiness score approximately the same in all the years?  Convert `year` to a factor variable, and fit a simple linear model to address this question.




```{r, version = "answer_key"}

whr_fit_2 <- lin_mod %>%
  fit(happiness_score ~ factor(year), data = whr)

whr_fit_2$fit %>% summary()
```
%%%
version: answer_key

Yes, the coefficients were significant for all the years, which suggests a different average happiness rating in each year as compared to the first year (2005)

%%%


#### Q2: Happiness Equation

How is each of the six measures of life quality weighted in calculating this score?  
Fit a model to estimate the weights, and interpret the coefficients.
Which measures of life quality does the WHR consider to be most important to a country's happiness?


*Hint: It is important to put all the metrics on the same scale first, since they are all in different units.*

```{r, version = "answer_key"}
whr_rec <- recipe(happiness_score ~ log_gdp_per_capita + healthy_life_expectancy + social_support +
        freedom_to_make_life_choices + generosity + perceptions_of_corruption,
        data = whr) %>%
  step_normalize(all_numeric(), -happiness_score)

whr_wflow <- workflow() %>%
  add_recipe(whr_rec) %>%
  add_model(lin_mod)

whr_fit <- whr_wflow %>%
  fit(whr) %>%
  pull_workflow_fit() 


whr_fit$fit %>%
  summary()

```

%%% 
version: answer_key

We first standardize all six variables, to make sure they are on the same scale regardless of how they are measured.

These six variables explain 74.86% of the variance in happiness scores; presumably the rest is changes to the WHR's formula from year to year.

The GDP per capita was the most important metric (coeff = 0.41).  Next were social support (0.275) and life expectancy (0.225), followed by freedom of choice (0.149), and last generosity and perception of corruption (both 0.118, with corruption being a detractor from happiness).

%%%



# Part Two:  Predicting life expectancy (linear model)

Suppose we would like to know how various quality-of-life measurements impact a 
country's life expectancy.  We plan to use the other five metrics, as well as the 
region of the country, to try to predict the life expectancy.

Explore many possible candidate models.  You should consider, within reason:

* Which variables should be included or omitted

* Possible interaction terms

* Polynomial terms of the variables (hint: you can use the function `step_poly(<variable name>, degree = 2)` to add polynomial terms in a `recipe`)

* *(optional)* Log-transformations or square root transformations of variables that appear skewed  (Hint: You can use the functions `step_log()` or `step_sqrt()` in a `recipe`)

Of course, I do **not** expect you to try every possible combination of the above.
Instead, you should come up with a reasonable approach that lets you eliminate many model
options without trying every single one.

Do **not** include the code and results for every single candidate you try in the writeup for this assignment.  Instead, supply the following:

#### Q1: Summary of approach

Write a short description (bullet points are fine) of your process in narrowing down your model.

How did you approach this problem, without spending hours upon hours fitting  and cross-validating zillions of models?

%%%
version: instructions

For example:

> I first fit models on each predictor by itself for polynomial 1 and polynomial 2, to determine if I should use any polynomial terms.  I found that [variable] should be polynomial 2 but the rest should not.  Then, I ...

%%%

%%%
version: answer_key

*(Answers will vary)*

My approach is to visualize each variable against the response.  For example, in the plots below, it appears that `perceptions_of_corruption` has a very weak relationship with life expectancy, but it might benefit from a polynomial transformation to spread the data out a bit.  `social_support` appears to have a strong relationship with the response.

%%%

```{r, version = "answer_key"}

whr %>%
  ggplot(aes(x = perceptions_of_corruption, y = healthy_life_expectancy)) +
  geom_point()

whr %>%
  ggplot(aes(x = perceptions_of_corruption^2, y = healthy_life_expectancy)) +
  geom_point()

whr %>%
  ggplot(aes(x = social_support, y = healthy_life_expectancy)) +
  geom_point()


```
%%%
version: answer_key

I then add variables to the model one by one, checking the R-squared at each fit.  When the R-squared changes hugely, I keep the variable.  When it changes only a little, I use cross-validation to see if the improvement is overfitting or meaningful.

%%%

#### Q2: Three example candidates

Choose the three best candidate models among those you tried.  

Supply your code and results for comparing these models, and discuss how you decided which *one* model was the best one.

```{r, version = "answer_key"}
whr_cv <- vfold_cv(whr, v = 5)

whr_rec_1 <- recipe(healthy_life_expectancy ~ 
                      log_gdp_per_capita + social_support + freedom_to_make_life_choices +
                      year + perceptions_of_corruption,
                    data = whr)

whr_rec_2 <- whr_rec_1 %>%
  step_naomit(perceptions_of_corruption, skip = TRUE) %>%
  step_poly(perceptions_of_corruption, degree = 2)


whr_rec_3 <- recipe(healthy_life_expectancy ~ 
                      log_gdp_per_capita + social_support + freedom_to_make_life_choices +
                      year,
                    data = whr) 

workflow() %>%
  add_model(lin_mod) %>%
  add_recipe(whr_rec_1) %>%
  fit_resamples(whr_cv) %>%
  collect_metrics()


workflow() %>%
  add_model(lin_mod) %>%
  add_recipe(whr_rec_2) %>%
  fit_resamples(whr_cv) %>%
  collect_metrics()

workflow() %>%
  add_model(lin_mod) %>%
  add_recipe(whr_rec_3) %>%
  fit_resamples(whr_cv) %>%
  collect_metrics()
```
%%%
version: answer_key

Based on the above, the best model is the one that **does** include perceptions of corruption.  It's almost exactly equal whether we square or not, so for simplicity, we'll use the non-polynomial model.
%%% 

#### Q3:  Final model

Summarize the results from your final model.  Don't forget to fit your final model
on the full dataset, even if you used *test/training* data or *cross-validation* 
during the model selection process.

Include a plot of the residuals and discussion of what you see, and interpretations of the coefficients and metrics.

```{r, version = "answer_key"}
whr_final <- workflow() %>%
  add_model(lin_mod) %>%
  add_recipe(whr_rec_1) %>%
  fit(whr)

whr_final$fit %>% summary()

whr %>%
  mutate(
    preds = predict(whr_final, whr)$.pred,
    resids = healthy_life_expectancy - preds
  ) %>%
  ggplot(aes(x = healthy_life_expectancy, y = resids)) +
  geom_point()
```

%%%
version: answer_key

The residuals are mostly a random cloud around zero, which is good.  However, there are some residuals at the lower end that seem to have a bit of a pattern, which is concerning - perhaps our models is missing some important information for the countries with lower life expectancies.

%%%

# Part Three: Predicting life expectancy (k-nearest-neighbors)

Now we will find the best KNN model for predicting life expectancy.

Consider only the three top candidate models from **Part Two**.

#### Q1: Tuning K

For **each** of your top three candidate models from Part Two, Q2, find the best
choice of **K**.  Show all your work, and provide a brief summary at the end.

%%%
version: instructions

> For Model 1, we choose a K of [something] because [reasons].  For Model 2, ...

%%%

```{r, version = "answer_key"}
whr_clean <- whr %>%
  drop_na()

whr_cv_clean <- vfold_cv(whr_clean, v = 5)

knn_spec_tune <- nearest_neighbor(neighbors = tune()) %>%
  set_mode("regression") %>%
  set_engine("kknn")

k_grid <- grid_regular(neighbors(c(2, 20)), 
                       levels = 5)

tune_1 <- workflow() %>%
  add_model(knn_spec_tune) %>%
  add_recipe(whr_rec_1) %>%
  tune_grid(
    resamples = whr_cv_clean,
    grid = k_grid
  ) %>%
  collect_metrics()

tune_1 %>% filter(.metric == "rmse") %>% arrange(.metric)
tune_1 %>% filter(.metric == "rsq") %>% arrange(desc(.metric))

tune_2 <- workflow() %>%
  add_model(knn_spec_tune) %>%
  add_recipe(whr_rec_2) %>%
  tune_grid(
    resamples = whr_cv_clean,
    grid = k_grid
  ) %>%
  collect_metrics()

tune_2 %>% filter(.metric == "rmse") %>% arrange(.metric)
tune_2 %>% filter(.metric == "rsq") %>% arrange(desc(.metric))

tune_3 <- workflow() %>%
  add_model(knn_spec_tune) %>%
  add_recipe(whr_rec_3) %>%
  tune_grid(
    resamples = whr_cv_clean,
    grid = k_grid
  ) %>%
  collect_metrics()

tune_3 %>% filter(.metric == "rmse") %>% arrange(.metric)
tune_3 %>% filter(.metric == "rsq") %>% arrange(desc(.metric))


```
%%%
version: answer_key

Model 1:  Lowest rmse and highest r-squared occurs at k = 15.

Model 2:  Lowest rmse occurs at k = 11 and highest r-squared occurs at k = 15.  We should check closer in this range if we choose this model.

Model 3:  Lowest rmse and highest r-squared occurs at k = 15.


Among these three, Model 1 again has the best performance.

%%%

#### Q2: Best model

Fit and report your single best model from Q1.

You should include:

* An argument for your choice of K, including a plot.

* A plot of the residuals

```{r, version = "answer_key"}

k_grid_finer <- grid_regular(neighbors(c(12, 19)), 
                       levels = 7)

tune_final <- workflow() %>%
  add_model(knn_spec_tune) %>%
  add_recipe(whr_rec_1) %>%
  tune_grid(
    resamples = whr_cv_clean,
    grid = k_grid_finer
  ) %>%
  collect_metrics()

tune_final %>% filter(.metric == "rmse") %>% arrange(.metric)
tune_final %>% filter(.metric == "rsq") %>% arrange(desc(.metric))

tune_final %>%
  ggplot(aes(x = neighbors, y = mean)) +
  geom_line() +
  facet_wrap(~.metric, scales = "free")

```
%%%
version: answer_key

Highest R-squared is at 15.  Lowest RMSE is at 14.  Let's go with 14 because I like R-squared.

%%%

```{r, version = "answer_key"}

knn_spec <- nearest_neighbor(neighbors = 14) %>%
  set_mode("regression") %>%
  set_engine("kknn")

final_knn <- workflow() %>%
  add_model(knn_spec) %>%
  add_recipe(whr_rec_1) %>%
  fit(whr_clean)

whr_clean %>%
  mutate(
    preds = predict(final_knn, whr_clean)$.pred,
    resids = healthy_life_expectancy - preds
  ) %>%
  ggplot(aes(x = preds, y = resids)) +
  geom_point()
```
%%%
version: answer_key

These residuals looks pretty good! They are around 0 and not strongly patterned.

(If we want to get fancy, there's some concern about heteroskedacisity, but this only matters in linear models.)

%%%


#### Q3: Different distance metrics

%%%
version: instructions

So far, we have only fit KNN models where distance between observations is measured
in **Euclidean distance**.  That is, if we have predictors $X_1, X_2, X_3$, then the 
distance between observations $i$ and $j$ is:

![](eq_euclidean.png)

This is not the only way to measure distance between observations!  Consider another
option, the **absolute distance**:

![](eq_absolute.png)

We can change the option of the distance metric in our model specification to be
"power of one", i.e., absolute values, instead of the default power of 2 for Euclidean distance:

%%%

```{r, version = "instructions", eval = FALSE}
knn_mod <- nearest_neighbor(k = 5, dist_power = 1)
```


Using the same predictors you chose in Q2, find the best choice of $K$ for a KNN model using absolute distance.  How does this model compare to the one from Q2?  Conceptually, why would you get a different answer using these two different distance metrics?

```{r, version = "answer_key"}
knn_spec_tune_m <- nearest_neighbor(neighbors = tune(), dist_power = 1) %>%
  set_mode("regression") %>%
  set_engine("kknn")

tune_m <- workflow() %>%
  add_model(knn_spec_tune_m) %>%
  add_recipe(whr_rec_1) %>%
  tune_grid(
    resamples = whr_cv_clean,
    grid = k_grid_finer
  ) %>%
  collect_metrics()

tune_m %>% filter(.metric == "rmse") %>% arrange(.metric)
tune_m %>% filter(.metric == "rsq") %>% arrange(desc(.metric))

tune_m %>%
  ggplot(aes(x = neighbors, y = mean)) +
  geom_line() +
  facet_wrap(~.metric, scales = "free")

```
%%%
version: answer_key

K = 14 is still looking good.

%%%

```{r, version = "answer_key"}
knn_spec_m <- nearest_neighbor(neighbors = 14, dist = 1) %>%
  set_mode("regression") %>%
  set_engine("kknn")

final_knn_m <- workflow() %>%
  add_model(knn_spec_m) %>%
  add_recipe(whr_rec_1) %>%
  fit(whr_clean)

whr_clean %>%
  mutate(
    preds = predict(final_knn_m, whr_clean)$.pred,
    resids = healthy_life_expectancy - preds
  ) %>%
  ggplot(aes(x = preds, y = resids)) +
  geom_point()
```


# Part Four: Predicting on new data

The following code will load up a dataset of the World Happiness Report from 2021, 
pertaining to happiness measurements in the year 2020.

```{r}
whr_new <- read_csv("https://www.dropbox.com/s/h4kgksgawui3n5z/whr_2020.csv?dl=1")
```
Use your **one** best *least-squares regression* model to predict the life expectancy of all countries.

```{r, version = "answer_key"}
final_mod_ls <- workflow() %>%
  add_model(lin_mod) %>%
  add_recipe(whr_rec_1) %>%
  fit(whr)

whr_new <- whr_new %>%
  mutate(
    preds_ls = predict(final_mod_ls, whr_new)$.pred
  )
```


Use your **one** best *KNN* model to predict the life expectancy of all countries.

```{r, version = "answer_key"}
final_mod_knn <- workflow() %>%
  add_model(knn_spec_m) %>%
  add_recipe(whr_rec_1) %>%
  fit(whr)

whr_new <- whr_new %>%
  mutate(
    preds_knn = predict(final_mod_knn, whr_new)$.pred
  )

whr_new %>%
  rmse(
    truth = healthy_life_expectancy,
    estimate = preds_ls
  )

whr_new %>%
  rmse(
    truth = healthy_life_expectancy,
    estimate = preds_knn
  )
```

Which model did a better job predicting the true values in the new data?

%%%
version: answer_key

The KNN model with absolute distance and K = 14 was the overall winner!

%%%

(We sometimes call this new data - that was not involved in the **cross-validation** process or in the final model fitting - the **validation** set.  It is the "Ultimate" test set, that only gets to be used once ever.)


# Part Five:  Discussion Questions

For these questions, you do __not__ have to actually perform any of the computations described.
However, if the conceptual answer is not obvious to you, it may help to do so!


#### Q1:  Parametric and Non-Parametric

Make an argument for why a **parametric** method, like least-squares regression, might be preferred in this task.  

%%%
version: answer_key

If we are trying to understand what factors drive a country's life expectancy, so that we can implement policy to improve well-being, the parametric model will be better. We'll be able to interpret the coefficients as measures of the relationship between predictors and response.

%%%

Then make an argument for why a **non-parametric** method, like K-Nearest-Neighbors, might be preferred.

%%%
version: answer_key

If we are trying to use current information to accurately predict the life expectancy next year in a particular country, it may be that a non-parametric method performs better.

%%%

%%%
version: instructions

Your arguments should be *conceptual*, not numeric.  That is, your answers should relate to the data context, assumptions, and goals; not the MSE or similar metrics that you calculated for any specific model fit.

%%%

#### Q2:  Interpretation and Prediction

If your only goal were **interpretation**, which of the candidate models (from *any* section of the assignment) would you have chosen?  Why?

%%%
version: answer_key

I would choose the linear model without the polynomial term, because transformed variables are harder to interpret and parametric methods are better for interpretation.

%%%

If your only goal were **prediction** on future data, which of the candidate models would you have chosen?  Why?

%%%
version: answer_key

In this case, I would choose KNN with K = 14, since it had the lowest MSE on the validation set and in cross-validation studies.

%%%


#### Q3:  Standardization

Consider your final best least-squares regression model from Part Two, Q3.
Suppose we fit this model again, but this time, we normalize **all** of the quantitative variables.

Will anything change?  Give a (conceptual) argument for why this is true.

%%%
version: answer_key

The coefficients will change, because when you normalize, you change the scale of the values of the predictors.

However, the significance of the coefficients and the performance of the model will not change, because you are not meaningfully changing the model assumptions in any way.

%%%


#### Q4:  Standardization II

Consider your final best KNN model from Part Three, Q3.
Suppose we fit this model again, but this time, we normalize **all** of the quantitative variables.

Will anything change?  Give a (conceptual) argument for why this is true.

%%%
version: answer_key

Absolutely!

(In fact, ideally we should have normalized all the quantitative variables from the start.)

KNN is based on distances between observations. If we do not normalize, variables measured on a large scale will weigh more heavily in the calculation.  Normalizing all the variables evens the playing field and changes the model assumptions, so the model performance will be different.

%%%


#### Q5: Quantitative or Categorical?

In Part One, Q1, you discovered that (spoiler alert!) the `year` variable did **not** have a linear relationship with the `happiness_score`, but that when we treated `year` as categorical, it did have some relationship.

Suppose we add the predictor `year` to our final model as a **categorical variable** and fit the model on all the data. Then, we use this new model to predict on the 2020 data.

What is wrong with this plan?

%%%
version: answer_key

If "year" is a categorical variable, we expect future data to be limited to the known categories.  But 2020 is not a category in the data up through 2019!

%%%

%%%
version: instructions


# Challenges:  

## Challenge 1: Weighting distances in KNN

The `nearest_neighbor()` model specification function has a `weight_func` argument.

This is used to weight the nearest-neighbor observations by the **distances** to the target observation; 
that is, instead of simply averaging the 5 closest neighbors, we would weight the "vote" from the first closest neighbor more strongly than the vote from the 5th.

Possible values of the `weight_func` argument are: "rectangular", "triangular", "epanechnikov", "biweight", "triweight", "cos", "inv", "gaussian", "rank", or "optimal".

For **10 Challenge Points**, try one these weightings on your best KNN model.  Describe the weighting scheme you chose (e.g., what does "rectangular" weighting mean?)  Was your weighted model better?


## Challenge 2:  Weighting predictors in KNN

Suppose we decided we wanted to weight the **predictors** in the KNN, so that some are given more importance (i.e, contribute more to the distance calculation) than others.

This is not a built-in argument to the `nearest_neighbor()` function, because it is not a *model specification* question.  

Instead, it is a *data pre-processing* question, since it is specific to the particular predictors of a particular dataset.  To weight predictors, we would simply multiply the more important ones by a larger number than the less important ones.

For **10 extra Challenge Points**:  Use the coefficients of a corresponding *least-squares* regression to weight the predictors in your *KNN* model.  Compare the results of this new KNN model to the unweighted one.  Which was better?

%%%